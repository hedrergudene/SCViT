{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.6.9 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "MedMNIST.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a4e336f086a64111bc09c6d9081fbc2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_303db54e7389400ca7b13fe28b5f66c0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_63dd893d5db14b41b218667b12a3db75",
              "IPY_MODEL_a3e433aaf81a4837bfb5a5c5fec5b823"
            ]
          }
        },
        "303db54e7389400ca7b13fe28b5f66c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "63dd893d5db14b41b218667b12a3db75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_9c4274f9bd1644dc8632e8c49da4f4a2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.02MB of 0.02MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e9b56c9ef61740318c9f87211b9fd5cc"
          }
        },
        "a3e433aaf81a4837bfb5a5c5fec5b823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b9fa678cbf844d7ebaf7b53e462bb036",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1d70205df7344693aead8905042a7679"
          }
        },
        "9c4274f9bd1644dc8632e8c49da4f4a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e9b56c9ef61740318c9f87211b9fd5cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b9fa678cbf844d7ebaf7b53e462bb036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1d70205df7344693aead8905042a7679": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de5fb7a391784b75ad1affbbea99a8ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bd1b18de341349999444126eb129d196",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1e0796f3e1d84d91b2886b76fe435a15",
              "IPY_MODEL_c4e5470400ba4a5c994c291f59dadb27"
            ]
          }
        },
        "bd1b18de341349999444126eb129d196": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e0796f3e1d84d91b2886b76fe435a15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_3e11dcabf2d34a5cbe9df8c72007e061",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.03MB of 0.03MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4aaa334495c042798e335c94c745af57"
          }
        },
        "c4e5470400ba4a5c994c291f59dadb27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3ec783b7f1ef49539726f61fae9bab9c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_725c653915524dde82130ee7a28bb956"
          }
        },
        "3e11dcabf2d34a5cbe9df8c72007e061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4aaa334495c042798e335c94c745af57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ec783b7f1ef49539726f61fae9bab9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "725c653915524dde82130ee7a28bb956": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hedrergudene/HViT_classification/blob/main/MedMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8GZS00OwNek"
      },
      "source": [
        "# 0 - Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64namXNWK98v",
        "outputId": "751e4cb3-d752-44f6-acd4-2b0b2735e942"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir macula && unzip /content/drive/MyDrive/archive.zip -d /content/macula >> /dev/null"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNMPWiZzwNeq",
        "outputId": "d7911356-8ede-4cdc-b742-eb68cac94140"
      },
      "source": [
        "!git clone https://benayas1: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@github.com/hedrergudene/HViT_classification.git\n",
        "!(cd /content/HViT_classification/ && python setup.py bdist_wheel && pip install dist/hvit-0.0.1-py3-none-any.whl) >> /dev/null\n",
        "!pip install -U tensorflow-addons >> /dev/null\n",
        "!pip install wandb >> /dev/null\n",
        "!pip install ptflops >> /dev/null\n",
        "!pip install timm >> /dev/null\n",
        "!pip install benatools >> /dev/null\n",
        "\n",
        "#!git clone https://github.com/MonashAI/HVT"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HViT_classification'...\n",
            "remote: Enumerating objects: 1040, done.\u001b[K\n",
            "remote: Counting objects: 100% (1040/1040), done.\u001b[K\n",
            "remote: Compressing objects: 100% (790/790), done.\u001b[K\n",
            "remote: Total 1040 (delta 567), reused 514 (delta 197), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1040/1040), 1.06 MiB | 4.30 MiB/s, done.\n",
            "Resolving deltas: 100% (567/567), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrU3JElRwNes",
        "outputId": "61594044-5e6b-49a2-d23a-a2349c895263"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "from typing import List, Dict\n",
        "import wandb\n",
        "# Import model\n",
        "from hvit.tf.ViT_model import HViT, ViT\n",
        "#from hvit.tf.train_medmnist import run_WB_experiment\n",
        "from hvit.tf.info import INFO\n",
        "from hvit.tf.evaluator import Evaluator\n",
        "import hvit.tf.dataset_without_pytorch as mdn\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Login into W&B\n",
        "WB_ENTITY = 'ual'\n",
        "WB_PROJECT = 'hvit_benchmark'\n",
        "WB_KEY = 'ab1f4c380e0a008223b6434a42907bacfd7b4e26'\n",
        "#WB_KEY = '1bb44e6be47564584868ec55bac8cf468cf0e47f'  # antonio's\n",
        "\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvv77FcaZrOZ"
      },
      "source": [
        "# 1 - Training loop function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyglL7QaZp4O"
      },
      "source": [
        "def load_data(dataclass, split, task, size, n_classes, n_channels, one_hot=True):\n",
        "    dataset = dataclass(split=split, download=True)\n",
        "    x = dataset.imgs\n",
        "    if size is not None:\n",
        "        x = np.stack([cv2.resize(img, (size,size), interpolation = cv2.INTER_AREA) for img in x])\n",
        "    if n_channels == 1:\n",
        "        #x = np.expand_dims(x, 3)\n",
        "        x = np.stack([x,x,x], axis=-1)\n",
        "    y = dataset.labels\n",
        "    if task == 'multi-class':\n",
        "        if one_hot:\n",
        "            y = tf.keras.utils.to_categorical(y, n_classes)\n",
        "    if task == 'binary-class':\n",
        "        y = np.squeeze(y, axis=1)\n",
        "    return x, y\n",
        "\n",
        "def run_WB_experiment(WB_KEY:str,\n",
        "                      WB_ENTITY:str,\n",
        "                      WB_PROJECT:str,\n",
        "                      WB_GROUP:str,\n",
        "                      model:tf.keras.Model,\n",
        "                      data_flag:str,\n",
        "                      ImageDataGenerator_config:Dict,\n",
        "                      flow_config:Dict,\n",
        "                      epochs:int=10,\n",
        "                      learning_rate:float=0.00005,\n",
        "                      weight_decay:float=0.0001,\n",
        "                      label_smoothing:float=.1,\n",
        "                      es_patience:int=10,\n",
        "                      verbose:int=1,\n",
        "                      resize:int = None,\n",
        "                      ):\n",
        "    # Check for GPU:\n",
        "    assert len(tf.config.list_physical_devices('GPU'))>0, f\"No GPU available. Check system settings.\"\n",
        "\n",
        "    monitor = 'val_AUC'\n",
        "    mode = 'max'\n",
        "\n",
        "    # Generators\n",
        "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**ImageDataGenerator_config['train'])\n",
        "    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**ImageDataGenerator_config['val'])\n",
        "    test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**ImageDataGenerator_config['test'])\n",
        "\n",
        "    if data_flag == 'macula':\n",
        "        task = 'multi-class'\n",
        "        n_classes = 4\n",
        "        monitor = 'val_loss'\n",
        "        mode = 'min'\n",
        "        train_generator = train_datagen.flow_from_directory('/content/macula/OCT2017 /train',\n",
        "                                                            target_size=(resize, resize),\n",
        "                                                            color_mode='rgb',\n",
        "                                                            class_mode='categorical',\n",
        "                                                            batch_size=flow_config['train']['batch_size'],\n",
        "                                                            shuffle=flow_config['train']['shuffle'],\n",
        "                                                            seed=flow_config['train']['seed'],\n",
        "                                                            )\n",
        "        val_generator = val_datagen.flow_from_directory('/content/macula/OCT2017 /val',\n",
        "                                                        target_size=(resize, resize),\n",
        "                                                        color_mode='rgb',\n",
        "                                                        class_mode='categorical',\n",
        "                                                        batch_size=flow_config['val']['batch_size'],\n",
        "                                                        shuffle=flow_config['val']['shuffle'],\n",
        "                                                        seed=flow_config['val']['seed'],\n",
        "                                                        )\n",
        "        test_generator = test_datagen.flow_from_directory('/content/macula/OCT2017 /test',\n",
        "                                                          target_size=(resize, resize),\n",
        "                                                          color_mode='rgb',\n",
        "                                                          class_mode='categorical',\n",
        "                                                          batch_size=flow_config['test']['batch_size'],\n",
        "                                                          shuffle=flow_config['test']['shuffle'],\n",
        "                                                          seed=flow_config['test']['seed'],\n",
        "                                                          )\n",
        "    else:\n",
        "        if data_flag == 'cifar100':\n",
        "            (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "            x_val, y_val = x_test, y_test\n",
        "\n",
        "        else:\n",
        "            # Download dataset\n",
        "            info = INFO[data_flag]\n",
        "            task = info['task']\n",
        "            n_channels = info['n_channels']\n",
        "            n_classes = len(info['label'])\n",
        "            n_classes = 1 if n_classes == 2 else n_classes\n",
        "\n",
        "            DataClass = getattr(mdn, info['python_class'])\n",
        "            print(f'Dataset {data_flag} Task {task} n_channels {n_channels} n_classes {n_classes}')\n",
        "\n",
        "            # load train Data\n",
        "            x_train, y_train = load_data(DataClass, 'train', task, resize, n_classes, n_channels)\n",
        "\n",
        "            # load val Data\n",
        "            x_val, y_val = load_data(DataClass, 'val', task, resize, n_classes, n_channels)\n",
        "\n",
        "            # load test Data\n",
        "            x_test, y_test = load_data(DataClass, 'test', task, resize, n_classes, n_channels)\n",
        "\n",
        "            print(f'X train {x_train.shape} | Y train {y_train.shape}')\n",
        "            print(f'X val {x_val.shape} | Y val {y_val.shape}')\n",
        "            print(f'X test {x_test.shape} | Y test {y_test.shape}')\n",
        "          \n",
        "            train_generator = train_datagen.flow(x=x_train, \n",
        "                                                y=y_train,\n",
        "                                                batch_size=flow_config['train']['batch_size'],\n",
        "                                                shuffle=flow_config['train']['shuffle'],\n",
        "                                                seed=flow_config['train']['seed'],\n",
        "                                                )\n",
        "            val_generator = val_datagen.flow(x=x_val,\n",
        "                                            y=y_val,\n",
        "                                            batch_size=flow_config['val']['batch_size'],\n",
        "                                            shuffle=flow_config['val']['shuffle'],\n",
        "                                            seed=flow_config['val']['seed'],\n",
        "                                            )\n",
        "            test_generator = test_datagen.flow(x=x_test,\n",
        "                                              y=y_test,\n",
        "                                              batch_size=flow_config['test']['batch_size'],\n",
        "                                              shuffle=flow_config['test']['shuffle'],\n",
        "                                              seed=flow_config['test']['seed'],\n",
        "                                              )\n",
        "    # Log in WB\n",
        "    wandb.login(key=WB_KEY)\n",
        "\n",
        "    # Train & validation steps\n",
        "    train_steps_per_epoch = len(train_generator)\n",
        "    val_steps_per_epoch = len(val_generator)\n",
        "    test_steps_per_epoch = len(test_generator)\n",
        "\n",
        "    # Save initial weights\n",
        "    #model.load_weights(os.path.join(os.getcwd(), 'model_weights.h5'))\n",
        "\n",
        "    # Credentials\n",
        "    wandb.init(project='_'.join([WB_PROJECT, data_flag]), entity=WB_ENTITY, group = WB_GROUP)\n",
        "    \n",
        "    # Model compile\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    if task == 'multi-class':\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing = label_smoothing)\n",
        "        metrics = [tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "                   tf.keras.metrics.AUC(multi_label=True, num_labels=n_classes, from_logits=True, name=\"AUC\"),\n",
        "                   tfa.metrics.F1Score(num_classes=n_classes, average='macro', name = 'f1_score')\n",
        "                   ]\n",
        "    if task == 'binary-class':\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing = label_smoothing)\n",
        "        metrics = [tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
        "                   tf.keras.metrics.AUC(multi_label=False, from_logits=True, name=\"AUC\")]\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=loss,\n",
        "        metrics=metrics,\n",
        "    )\n",
        "\n",
        "    # Callbacks\n",
        "    reduceLR = tf.keras.callbacks.ReduceLROnPlateau(monitor=monitor, mode=mode, factor=0.2, patience=int(es_patience/2), min_lr=learning_rate//100, verbose=1)\n",
        "    patience = tf.keras.callbacks.EarlyStopping(monitor=monitor, mode=mode, patience=es_patience)\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(os.path.join(os.getcwd(), 'model_best_weights.h5'), monitor=monitor, mode=mode, save_best_only = True, save_weights_only = True)\n",
        "    wandb_callback = wandb.keras.WandbCallback(save_weights_only=True)\n",
        "\n",
        "    # Model fit\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch= train_steps_per_epoch,\n",
        "        epochs = epochs,\n",
        "        validation_data=val_generator,\n",
        "        validation_steps = val_steps_per_epoch,\n",
        "        callbacks=[reduceLR, patience, checkpoint, wandb_callback],\n",
        "        verbose = verbose,\n",
        "    )\n",
        "\n",
        "    # Evaluation\n",
        "    model.load_weights(os.path.join(os.getcwd(), 'model_best_weights.h5'))\n",
        "    results = model.evaluate(test_generator, steps = test_steps_per_epoch, verbose = 0)\n",
        "    print(\"Test metrics:\",{k:v for k,v in zip(model.metrics_names, results)})\n",
        "    wandb.log({(\"test_\"+k):v for k,v in zip(model.metrics_names, results)})\n",
        "    wandb.log({\"n_parameters\":np.round(model.count_params()/1000000, 1)})\n",
        "\n",
        "    #y_pred = model.predict(test_generator, verbose = 0)\n",
        "    #evaluator = Evaluator(data_flag, 'test')\n",
        "    #results = evaluator.evaluate(y_pred)\n",
        "\n",
        "    #print(f\"Test metrics: AUC {results.AUC}, ACC {results.ACC}\")\n",
        "    #wandb.log({\"test_ACC\":results.ACC, \"test_AUC\":results.AUC})\n",
        "\n",
        "    # Clear memory\n",
        "    tf.keras.backend.clear_session()\n",
        "    wandb.finish()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N034mIbwfSY"
      },
      "source": [
        "# 2 - Global Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP99yr--wnkI"
      },
      "source": [
        "# Config\n",
        "# 'pneumoniamnist','breastmnist'\n",
        "#datasets = ['octmnist','tissuemnist','pathmnist','dermamnist','bloodmnist', 'organamnist', 'organcmnist', 'organsmnist']\n",
        "datasets = ['bloodmnist']\n",
        "#datasets = ['macula']\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "es_patience = 7\n",
        "seed = 7853\n",
        "verbose=1\n",
        "learning_rate = 0.0001\n",
        "weight_decay = 0.0001\n",
        "label_smoothing = .1\n",
        "img_size = 32\n",
        "\n",
        "ImageDataGenerator_config = {\n",
        "    'train':{\n",
        "        \"rescale\":1./255,\n",
        "        \"shear_range\":.1,\n",
        "        \"rotation_range\":.2,\n",
        "        \"zoom_range\":.1,\n",
        "        \"horizontal_flip\" : True,\n",
        "        },\n",
        "    'val':{\n",
        "        \"rescale\":1./255,\n",
        "        },\n",
        "    'test':{\n",
        "        \"rescale\":1./255,\n",
        "        }\n",
        "}\n",
        "flow_config = {\n",
        "    'train':{\n",
        "        \"batch_size\":batch_size,\n",
        "        \"shuffle\":True,\n",
        "        \"seed\":seed,\n",
        "        },\n",
        "    'val':{\n",
        "        \"batch_size\":batch_size,\n",
        "        \"shuffle\":False,\n",
        "        \"seed\":seed,\n",
        "        },\n",
        "    'test':{\n",
        "        \"batch_size\":batch_size,\n",
        "        \"shuffle\":False,\n",
        "        \"seed\":seed,\n",
        "        }\n",
        "}"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW71oW4MwNeu"
      },
      "source": [
        "# 3 - Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEfKwXGdwNez"
      },
      "source": [
        "## HViT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OR--_LKwNe0"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "WB_GROUP = 'HViT'\n",
        "\n",
        "for data_flag in datasets:\n",
        "\n",
        "    hvit_params = { 'img_size':img_size,\n",
        "                    'patch_size':[2,4,8],\n",
        "                    'num_channels': 3,\n",
        "                    'num_heads': 8,\n",
        "                    'transformer_layers':[4,4,4],\n",
        "                    'hidden_unit_factor':2,\n",
        "                    'mlp_head_units': [256, 64],\n",
        "                    'num_classes':n_classes,\n",
        "                    'drop_attn':0.2,\n",
        "                    'drop_proj':0.2,\n",
        "                    'drop_linear':0.4,\n",
        "                    'projection_dim' : 48,\n",
        "                    'resampling_type':\"conv\",\n",
        "                    'original_attn':True,\n",
        "                    }\n",
        "\n",
        "    if data_flag in INFO:\n",
        "        info = INFO[data_flag]\n",
        "        n_classes = len(info['label'])\n",
        "        n_classes = 1 if n_classes == 2 else n_classes\n",
        "    else:\n",
        "        if data_flag == 'cifar100':\n",
        "            n_classes = 100\n",
        "        else:\n",
        "            n_classes = 4\n",
        "            hvit_params = { 'img_size':128,\n",
        "                    'patch_size':[8,16,32],\n",
        "                    'num_channels': 3,\n",
        "                    'num_heads': 8,\n",
        "                    'transformer_layers':[4,4,4],\n",
        "                    'hidden_unit_factor':2,\n",
        "                    'mlp_head_units': [256, 64],\n",
        "                    'num_classes':n_classes,\n",
        "                    'drop_attn':0.2,\n",
        "                    'drop_proj':0.2,\n",
        "                    'drop_linear':0.4,\n",
        "                    'projection_dim' : 768,\n",
        "                    'resampling_type':\"conv\",\n",
        "                    'original_attn':True,\n",
        "                    }\n",
        "\n",
        "    # Start running\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      # Instance model\n",
        "      inputs = tf.keras.layers.Input((img_size, img_size, 3))\n",
        "      outputs = HViT(**hvit_params)(inputs)\n",
        "      model = tf.keras.Model(inputs, outputs)\n",
        "      # Run experiment\n",
        "      run_WB_experiment(WB_KEY,\n",
        "                        WB_ENTITY,\n",
        "                        WB_PROJECT,\n",
        "                        WB_GROUP,\n",
        "                        model,\n",
        "                        data_flag,\n",
        "                        ImageDataGenerator_config,\n",
        "                        flow_config,\n",
        "                        epochs=epochs,\n",
        "                        learning_rate=learning_rate,\n",
        "                        weight_decay=weight_decay,\n",
        "                        label_smoothing = label_smoothing,\n",
        "                        verbose=verbose,\n",
        "                        resize=img_size,\n",
        "                        es_patience=es_patience,\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmj5bHpUsP1H"
      },
      "source": [
        "## ViT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gto-rgFwswDR"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "WB_GROUP = 'ViT'\n",
        "\n",
        "for data_flag in datasets:\n",
        "\n",
        "    if data_flag in INFO:\n",
        "        info = INFO[data_flag]\n",
        "        n_classes = len(info['label'])\n",
        "        n_classes = 1 if n_classes == 2 else n_classes\n",
        "        vit_params = {'img_size':img_size,\n",
        "                  'patch_size':4,\n",
        "                  'num_channels': 3,\n",
        "                  'num_heads': 8,\n",
        "                  'transformer_layers':16,\n",
        "                  'hidden_unit_factor':4,\n",
        "                  'mlp_head_units': [256, 64],\n",
        "                  'num_classes':n_classes,\n",
        "                  'drop_attn':0.2,\n",
        "                  'drop_proj':0.2,\n",
        "                  'drop_linear':0.4,\n",
        "                  'projection_dim' : 3*16\n",
        "                  }\n",
        "    else:\n",
        "        if data_flag == 'cifar100':\n",
        "            n_classes = 100\n",
        "            vit_params = {'img_size':img_size,\n",
        "                      'patch_size':4,\n",
        "                      'num_channels': 3,\n",
        "                      'num_heads': 8,\n",
        "                      'transformer_layers':16,\n",
        "                      'hidden_unit_factor':4,\n",
        "                      'mlp_head_units': [256, 64],\n",
        "                      'num_classes':n_classes,\n",
        "                      'drop_attn':0.2,\n",
        "                      'drop_proj':0.2,\n",
        "                      'drop_linear':0.4,\n",
        "                      'projection_dim' : 3*16\n",
        "                      }\n",
        "        else:\n",
        "            n_classes = 4\n",
        "            vit_params = {'img_size':img_size,  # 128\n",
        "                          'patch_size':16,\n",
        "                          'num_channels': 3,\n",
        "                          'num_heads': 8,\n",
        "                          'transformer_layers':12,\n",
        "                          'hidden_unit_factor':2,\n",
        "                          'mlp_head_units': [256, 64],\n",
        "                          'num_classes':n_classes,\n",
        "                          'drop_attn':0.2,\n",
        "                          'drop_proj':0.2,\n",
        "                          'drop_linear':0.4,\n",
        "                          'projection_dim' : 768,\n",
        "                          'resampling_type':\"conv\",\n",
        "                          'original_attn':True,\n",
        "                          }\n",
        "\n",
        "    # Start running\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        # Instance model\n",
        "        inputs = tf.keras.layers.Input((img_size, img_size, 3))\n",
        "        outputs = ViT(**vit_params)(inputs)\n",
        "        model = tf.keras.Model(inputs, outputs)\n",
        "        # Run experiment\n",
        "        run_WB_experiment(WB_KEY,\n",
        "                          WB_ENTITY,\n",
        "                          WB_PROJECT,\n",
        "                          WB_GROUP,\n",
        "                          model,\n",
        "                          data_flag,\n",
        "                          ImageDataGenerator_config,\n",
        "                          flow_config,\n",
        "                          epochs=epochs,\n",
        "                          learning_rate=learning_rate,\n",
        "                          weight_decay=weight_decay,\n",
        "                          label_smoothing = label_smoothing,\n",
        "                          verbose=verbose,\n",
        "                          resize=img_size,\n",
        "                          es_patience=es_patience,\n",
        "                          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WIq-kkGzCNZ"
      },
      "source": [
        "## EfficientNetB0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQFWFM8tzBc7"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "WB_GROUP = 'EfficientNetB0'\n",
        "mlp_head_units = [256,64]\n",
        "drop_linear = .2\n",
        "\n",
        "for data_flag in datasets:\n",
        "\n",
        "    if data_flag in INFO:\n",
        "        info = INFO[data_flag]\n",
        "        n_classes = len(info['label'])\n",
        "        n_classes = 1 if n_classes == 2 else n_classes\n",
        "    else:\n",
        "        if data_flag == 'cifar100':\n",
        "            n_classes = 100\n",
        "        else:\n",
        "            n_classes = 4\n",
        "\n",
        "    # Start running\n",
        "    with tf.device('/device:GPU:0'):\n",
        "\n",
        "      # Instance model\n",
        "      inputs = tf.keras.layers.Input((img_size, img_size, 3))\n",
        "      base_model = tf.keras.applications.EfficientNetB0(weights=None, include_top=False)(inputs)\n",
        "      x = tf.keras.layers.GlobalAveragePooling2D()(base_model)\n",
        "      for i in mlp_head_units:\n",
        "          x = tf.keras.layers.Dense(i)(x)\n",
        "          x = tf.keras.layers.Dropout(drop_linear)(x)\n",
        "      logits = tf.keras.layers.Dense(n_classes)(x)\n",
        "      model = tf.keras.Model(inputs, logits)\n",
        "\n",
        "      # Run experiment\n",
        "      run_WB_experiment(WB_KEY,\n",
        "                        WB_ENTITY,\n",
        "                        WB_PROJECT,\n",
        "                        WB_GROUP,\n",
        "                        model,\n",
        "                        data_flag,\n",
        "                        ImageDataGenerator_config,\n",
        "                        flow_config,\n",
        "                        epochs=epochs,\n",
        "                        learning_rate=learning_rate,\n",
        "                        weight_decay=weight_decay,\n",
        "                        label_smoothing = label_smoothing,\n",
        "                        verbose=verbose,\n",
        "                        resize=img_size,\n",
        "                        es_patience=es_patience,\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUnseswN_jZe"
      },
      "source": [
        "## EfficientNetB4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6To5LCO0_ieW"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "WB_GROUP = 'EfficientNetB4'\n",
        "mlp_head_units = [256,64]\n",
        "drop_linear = .2\n",
        "\n",
        "for data_flag in datasets:\n",
        "\n",
        "    if data_flag in INFO:\n",
        "        info = INFO[data_flag]\n",
        "        n_classes = len(info['label'])\n",
        "        n_classes = 1 if n_classes == 2 else n_classes\n",
        "    else:\n",
        "        if data_flag == 'cifar100':\n",
        "            n_classes = 100\n",
        "        else:\n",
        "            n_classes = 4\n",
        "\n",
        "    # Start running\n",
        "    with tf.device('/device:GPU:0'):\n",
        "\n",
        "      # Instance model\n",
        "      inputs = tf.keras.layers.Input((img_size, img_size, 3))\n",
        "      base_model = tf.keras.applications.EfficientNetB4(weights=None, include_top=False)(inputs)\n",
        "      x = tf.keras.layers.GlobalAveragePooling2D()(base_model)\n",
        "      for i in mlp_head_units:\n",
        "          x = tf.keras.layers.Dense(i)(x)\n",
        "          x = tf.keras.layers.Dropout(drop_linear)(x)\n",
        "      logits = tf.keras.layers.Dense(n_classes)(x)\n",
        "      model = tf.keras.Model(inputs, logits)\n",
        "\n",
        "      # Run experiment\n",
        "      run_WB_experiment(WB_KEY,\n",
        "                        WB_ENTITY,\n",
        "                        WB_PROJECT,\n",
        "                        WB_GROUP,\n",
        "                        model,\n",
        "                        data_flag,\n",
        "                        ImageDataGenerator_config,\n",
        "                        flow_config,\n",
        "                        epochs=epochs,\n",
        "                        learning_rate=learning_rate,\n",
        "                        weight_decay=weight_decay,\n",
        "                        label_smoothing = label_smoothing,\n",
        "                        verbose=verbose,\n",
        "                        resize=img_size,\n",
        "                        es_patience=es_patience,\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMgBP9xY2dus"
      },
      "source": [
        "## ResNet 150v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7jBSm952mDN"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "WB_GROUP = \"ResNet 152 v2\"\n",
        "mlp_head_units = [256,64]\n",
        "drop_linear = .2\n",
        "\n",
        "for data_flag in datasets:\n",
        "\n",
        "    if data_flag in INFO:\n",
        "        info = INFO[data_flag]\n",
        "        n_classes = len(info['label'])\n",
        "        n_classes = 1 if n_classes == 2 else n_classes\n",
        "    else:\n",
        "        if data_flag == 'cifar100':\n",
        "            n_classes = 100\n",
        "        else:\n",
        "            n_classes = 4\n",
        "\n",
        "    # Start running\n",
        "    with tf.device('/device:GPU:0'):\n",
        "\n",
        "      # Instance model\n",
        "      inputs = tf.keras.layers.Input((img_size, img_size, 3))\n",
        "      base_model = tf.keras.applications.resnet_v2.ResNet152V2(weights=None, include_top=False)(inputs)\n",
        "      x = tf.keras.layers.GlobalAveragePooling2D()(base_model)\n",
        "      for i in mlp_head_units:\n",
        "          x = tf.keras.layers.Dense(i)(x)\n",
        "          x = tf.keras.layers.Dropout(drop_linear)(x)\n",
        "      logits = tf.keras.layers.Dense(n_classes)(x)\n",
        "      model = tf.keras.Model(inputs, logits)\n",
        "\n",
        "      # Run experiment\n",
        "      run_WB_experiment(WB_KEY,\n",
        "                        WB_ENTITY,\n",
        "                        WB_PROJECT,\n",
        "                        WB_GROUP,\n",
        "                        model,\n",
        "                        data_flag,\n",
        "                        ImageDataGenerator_config,\n",
        "                        flow_config,\n",
        "                        epochs=epochs,\n",
        "                        learning_rate=learning_rate,\n",
        "                        weight_decay=weight_decay,\n",
        "                        label_smoothing = label_smoothing,\n",
        "                        verbose=verbose,\n",
        "                        resize=img_size,\n",
        "                        es_patience=es_patience,\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0PT7AGvWvYc"
      },
      "source": [
        "## Conv Mixer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjBIdeetWxme"
      },
      "source": [
        "def activation_block(x, dropout=.2):\n",
        "    x = tf.keras.layers.Activation(\"gelu\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(dropout)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def conv_stem(x, filters: int, patch_size: int, dropout: float):\n",
        "    x = tf.keras.layers.Conv2D(filters, kernel_size=patch_size, strides=patch_size)(x)\n",
        "    return activation_block(x, dropout)\n",
        "\n",
        "\n",
        "def conv_mixer_block(x, filters: int, kernel_size: int, dropout: float):\n",
        "    # Depthwise convolution.\n",
        "    x0 = x\n",
        "    x = tf.keras.layers.DepthwiseConv2D(kernel_size=kernel_size, padding=\"same\")(x)\n",
        "    x = tf.keras.layers.Add()([activation_block(x, dropout), x0])  # Residual.\n",
        "\n",
        "    # Pointwise convolution.\n",
        "    x = tf.keras.layers.Conv2D(filters, kernel_size=1)(x)\n",
        "    x = activation_block(x, dropout)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_conv_mixer_256_8(\n",
        "    image_size=32, filters=256, depth=12, kernel_size=5, patch_size=4, mlp_head_units:List[int]=[256,64], drop_enc:float=.2, drop_linear:float=.2, num_classes=10,\n",
        "):\n",
        "    \"\"\"ConvMixer-256/8: https://openreview.net/pdf?id=TVHS5Y4dNvM.\n",
        "    The hyperparameter values are taken from the paper.\n",
        "    \"\"\"\n",
        "    inputs = tf.keras.Input((image_size, image_size, 3))\n",
        "    x = tf.keras.layers.Rescaling(scale=1.0 / 255)(inputs)\n",
        "\n",
        "    # Extract patch embeddings.\n",
        "    x = conv_stem(x, filters, patch_size, drop_enc)\n",
        "    # ConvMixer blocks.\n",
        "    for _ in range(depth):\n",
        "        x = conv_mixer_block(x, filters, kernel_size, drop_enc)\n",
        "\n",
        "    # Classification block.\n",
        "    x = tf.keras.layers.GlobalAvgPool2D()(x)\n",
        "    for i in mlp_head_units:\n",
        "        x = tf.keras.layers.Dense(i)(x)\n",
        "        x = tf.keras.layers.Dropout(drop_linear)(x)\n",
        "    logits = tf.keras.layers.Dense(num_classes)(x)\n",
        "    return tf.keras.Model(inputs, logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSVXlsCiW03f"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "WB_GROUP = \"ConvMixer\"\n",
        "mlp_head_units = [256,64]\n",
        "drop_linear = .2\n",
        "\n",
        "for data_flag in datasets:\n",
        "\n",
        "    if data_flag in INFO:\n",
        "        info = INFO[data_flag]\n",
        "        n_classes = len(info['label'])\n",
        "        n_classes = 1 if n_classes == 2 else n_classes\n",
        "    else:\n",
        "        if data_flag == 'cifar100':\n",
        "            n_classes = 100\n",
        "        else:\n",
        "            n_classes = 4\n",
        "\n",
        "    # Start running\n",
        "    with tf.device('/device:GPU:0'):\n",
        "\n",
        "      # Instance model\n",
        "      model = get_conv_mixer_256_8(patch_size = 4, num_classes = n_classes)\n",
        "\n",
        "      # Run experiment\n",
        "      run_WB_experiment(WB_KEY,\n",
        "                        WB_ENTITY,\n",
        "                        WB_PROJECT,\n",
        "                        WB_GROUP,\n",
        "                        model,\n",
        "                        data_flag,\n",
        "                        ImageDataGenerator_config,\n",
        "                        flow_config,\n",
        "                        epochs=epochs,\n",
        "                        learning_rate=learning_rate,\n",
        "                        weight_decay=weight_decay,\n",
        "                        label_smoothing = label_smoothing,\n",
        "                        verbose=verbose,\n",
        "                        resize=img_size,\n",
        "                        es_patience=es_patience,\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLDFy4Af3g_P"
      },
      "source": [
        "## Inception ResNet v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGnIY2iv3mXI"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "WB_GROUP = \"Inception ResNet v2\"\n",
        "mlp_head_units = [256,64]\n",
        "drop_linear = .2\n",
        "\n",
        "img_size=128\n",
        "for data_flag in datasets:\n",
        "\n",
        "    info = INFO[data_flag]\n",
        "    n_classes = len(info['label'])\n",
        "    n_classes = 1 if n_classes == 2 else n_classes\n",
        "\n",
        "    # Start running\n",
        "    with tf.device('/device:GPU:0'):\n",
        "\n",
        "      # Instance model\n",
        "      inputs = tf.keras.layers.Input((img_size, img_size, 3))\n",
        "      x = tf.keras.applications.inception_resnet_v2.preprocess_input(inputs)\n",
        "      base_model = tf.keras.applications.InceptionResNetV2(weights=None, include_top=False)(x)\n",
        "      x = tf.keras.layers.GlobalAveragePooling2D()(base_model)\n",
        "      for i in mlp_head_units:\n",
        "          x = tf.keras.layers.Dense(i)(x)\n",
        "          x = tf.keras.layers.Dropout(drop_linear)(x)\n",
        "      logits = tf.keras.layers.Dense(n_classes)(x)\n",
        "      model = tf.keras.Model(inputs, logits)\n",
        "\n",
        "      # Run experiment\n",
        "      run_WB_experiment(WB_KEY,\n",
        "                        WB_ENTITY,\n",
        "                        WB_PROJECT,\n",
        "                        WB_GROUP,\n",
        "                        model,\n",
        "                        data_flag,\n",
        "                        ImageDataGenerator_config,\n",
        "                        flow_config,\n",
        "                        epochs=epochs,\n",
        "                        learning_rate=learning_rate,\n",
        "                        weight_decay=weight_decay,\n",
        "                        label_smoothing = label_smoothing,\n",
        "                        verbose=verbose,\n",
        "                        resize=img_size,\n",
        "                        es_patience=es_patience,\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE6njEsl4MDE"
      },
      "source": [
        "## HVT (PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxnlpz8xyDkf"
      },
      "source": [
        "import torch\n",
        "from benatools.torch.fitter import TorchFitterBase\n",
        "from hvit.pytorch.HVT.models import hvt_model\n",
        "from hvit.pytorch.HVT.params import args\n",
        "from hvit.pytorch.HVT.datasets import build_transform\n",
        "from torchvision import transforms\n",
        "\n",
        "import datetime\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "from timm.loss import LabelSmoothingCrossEntropy\n",
        "from hvit.pytorch.HVT.models import hvt_model\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "args['input-size'] = img_size\n",
        "\n",
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, label, augments=None, im_size=32):\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "        self.augments = augments\n",
        "        self.im_size = im_size\n",
        "        self.toTensor = transforms.ToTensor()\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        x = cv2.resize(self.data[idx], (self.im_size, self.im_size))\n",
        "        y = self.label[idx].astype(np.int64)\n",
        "        \n",
        "        # Augmentation including scaling\n",
        "        if self.augments:\n",
        "            x = self.augments(x)\n",
        "        \n",
        "        x = x/255.\n",
        "\n",
        "        return {'x':x, 'y':y}\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "class ImageFitter(TorchFitterBase):\n",
        "    def unpack(self, data):\n",
        "        # extract x and y from the dataloader\n",
        "        x = data['x'].to(self.device).float()\n",
        "        y = data['y'].to(self.device)\n",
        "        y = torch.squeeze(y)\n",
        "\n",
        "        # weights if existing\n",
        "        return x, y, None\n",
        "\n",
        "def get_transform(is_train):\n",
        "    if is_train:\n",
        "        return transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.RandomAffine(.2, shear=.1,),\n",
        "                    transforms.RandomHorizontalFlip(p=0.5)\n",
        "                ])\n",
        "    else:\n",
        "        return transforms.ToTensor()\n",
        "\n",
        "def get_data(data_flag, resize=32):\n",
        "    # Download dataset\n",
        "    info = INFO[data_flag]\n",
        "    task = info['task']\n",
        "    n_channels = info['n_channels']\n",
        "    n_classes = len(info['label'])\n",
        "    n_classes = 1 if n_classes == 2 else n_classes\n",
        "\n",
        "    DataClass = getattr(mdn, info['python_class'])\n",
        "    print(f'Dataset {data_flag} Task {task} n_channels {n_channels} n_classes {n_classes}')\n",
        "\n",
        "    # load train Data\n",
        "    x_train, y_train = load_data(DataClass, 'train', task, resize, n_classes, n_channels, one_hot=False)\n",
        "\n",
        "    # load val Data\n",
        "    x_val, y_val = load_data(DataClass, 'val', task, resize, n_classes, n_channels, one_hot=False)\n",
        "\n",
        "    # load test Data\n",
        "    x_test, y_test = load_data(DataClass, 'test', task, resize, n_classes, n_channels, one_hot=False)\n",
        "\n",
        "    print(f'X train {x_train.shape} | Y train {y_train.shape}')\n",
        "    print(f'X val {x_val.shape} | Y val {y_val.shape}')\n",
        "    print(f'X test {x_test.shape} | Y test {y_test.shape}')\n",
        "\n",
        "          \n",
        "    train_generator = ImageDataset(x_train, y_train, get_transform(is_train=True), resize)\n",
        "    val_generator = ImageDataset(x_val, y_val, get_transform(is_train=False), resize)\n",
        "    test_generator = ImageDataset(x_test, y_test, get_transform(is_train=False), resize)\n",
        "\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_generator,\n",
        "                                                   batch_size=flow_config['train']['batch_size'],\n",
        "                                                   shuffle=flow_config['train']['shuffle'],\n",
        "                                                   num_workers=2\n",
        "                                                   )\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_generator,\n",
        "                                                 batch_size=flow_config['val']['batch_size'],\n",
        "                                                 shuffle=flow_config['val']['shuffle'],\n",
        "                                                 num_workers=2,\n",
        "                                                 )\n",
        "    test_dataloader = torch.utils.data.DataLoader(test_generator,\n",
        "                                                  batch_size=flow_config['test']['batch_size'],\n",
        "                                                  shuffle=flow_config['test']['shuffle'],\n",
        "                                                  num_workers=2,\n",
        "                                                  )\n",
        "    \n",
        "    return train_dataloader, val_dataloader, test_dataloader\n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a4e336f086a64111bc09c6d9081fbc2f",
            "303db54e7389400ca7b13fe28b5f66c0",
            "63dd893d5db14b41b218667b12a3db75",
            "a3e433aaf81a4837bfb5a5c5fec5b823",
            "9c4274f9bd1644dc8632e8c49da4f4a2",
            "e9b56c9ef61740318c9f87211b9fd5cc",
            "b9fa678cbf844d7ebaf7b53e462bb036",
            "1d70205df7344693aead8905042a7679",
            "de5fb7a391784b75ad1affbbea99a8ab",
            "bd1b18de341349999444126eb129d196",
            "1e0796f3e1d84d91b2886b76fe435a15",
            "c4e5470400ba4a5c994c291f59dadb27",
            "3e11dcabf2d34a5cbe9df8c72007e061",
            "4aaa334495c042798e335c94c745af57",
            "3ec783b7f1ef49539726f61fae9bab9c",
            "725c653915524dde82130ee7a28bb956"
          ]
        },
        "id": "ltt4DKk1qkEa",
        "outputId": "333bd930-8abc-4abd-e000-aba372701157"
      },
      "source": [
        "def wandb_update(x):\n",
        "    data_log = x.copy()\n",
        "    del data_log['epoch']\n",
        "    wandb.log({'training':data_log})\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "WB_GROUP = \"HVT-small\"\n",
        "mlp_head_units = [256,64]\n",
        "drop_linear = .2\n",
        "learning_rate = 5e-4\n",
        "\n",
        "if WB_GROUP == \"HVT-small\":\n",
        "    model_params = { #small\n",
        "      \"model\": \"hvt_model\",\n",
        "      \"batch_size\": batch_size,\n",
        "      \"exp_name\": \"hvt-s-1\",\n",
        "      \"input_size\": img_size, #224,\n",
        "      \"patch_size\": 16,\n",
        "      \"num_heads\": 6,\n",
        "      \"head_dim\": 64,\n",
        "      \"num_blocks\": 12,\n",
        "      \"num_workers\": 10,\n",
        "      \"pool_kernel_size\": 3,\n",
        "      \"pool_stride\": 2,\n",
        "      \"pool_block_width\": 12,\n",
        "      \"weight_decay\": 0.025\n",
        "    }\n",
        "else:\n",
        "    model_params = {\n",
        "      \"model\": \"hvt_model\",\n",
        "      \"batch_size\": batch_size,\n",
        "      \"exp_name\": \"hvt-ti-1\",\n",
        "      \"input_size\": img_size, #224,\n",
        "      \"patch_size\": 16,\n",
        "      \"num_heads\": 3,\n",
        "      \"head_dim\": 64,\n",
        "      \"num_blocks\": 12,\n",
        "      \"num_workers\": 10,\n",
        "      \"pool_kernel_size\": 3,\n",
        "      \"pool_stride\": 2,\n",
        "      \"pool_block_width\": 12,\n",
        "      \"weight_decay\": 0.025\n",
        "    }\n",
        "\n",
        "device = torch.device(args['device'])\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "for data_flag in datasets:\n",
        "\n",
        "    if data_flag in INFO:\n",
        "        info = INFO[data_flag]\n",
        "        n_classes = len(info['label'])\n",
        "        n_classes = 1 if n_classes == 2 else n_classes\n",
        "    else:\n",
        "        if data_flag == 'cifar100':\n",
        "            n_classes = 100\n",
        "        else:\n",
        "            n_classes = 4\n",
        "\n",
        "    train_loader, val_loader, test_loader = get_data(data_flag, img_size)\n",
        "\n",
        "    wandb.login(key=WB_KEY)\n",
        "    wandb.init(project='_'.join([WB_PROJECT, data_flag]), entity=WB_ENTITY, group = WB_GROUP)\n",
        "\n",
        "    model = hvt_model(\n",
        "        pretrained=False,\n",
        "        head_dim=model_params['head_dim'],\n",
        "        num_heads=model_params['num_heads'],\n",
        "        input_size=model_params['input_size'],\n",
        "        patch_size=model_params['patch_size'],\n",
        "        num_blocks=model_params['num_blocks'],\n",
        "        pool_block_width=model_params['pool_block_width'],\n",
        "        pool_kernel_size=model_params['pool_kernel_size'],\n",
        "        num_classes=n_classes,\n",
        "        drop_rate=drop_linear,\n",
        "        drop_path_rate=0.1,\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    print(str(model))\n",
        "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print('number of params: ' + str(n_parameters))\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-8, weight_decay=0.01)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=int(es_patience/2), min_lr=learning_rate//100, verbose=1)\n",
        "    criterion = LabelSmoothingCrossEntropy(smoothing=label_smoothing)\n",
        " \n",
        "    fitter = ImageFitter(model,\n",
        "                         loss=criterion,\n",
        "                         optimizer=optimizer,\n",
        "                         scheduler = lr_scheduler,\n",
        "                         device=device,\n",
        "                         folder='models_HVT',\n",
        "                         use_amp=False)\n",
        "    \n",
        "    history = fitter.fit(train_loader,\n",
        "                         val_loader=val_loader,\n",
        "                         n_epochs=epochs, \n",
        "                         early_stopping=es_patience,\n",
        "                         early_stopping_mode='max',\n",
        "                         metrics = [(f1_score, {'average':'macro'}), (accuracy_score, {})], \n",
        "                         save_checkpoint=False,\n",
        "                         save_best_checkpoint=True,\n",
        "                         verbose_steps=5,\n",
        "                         callbacks=[wandb_update])\n",
        "    \n",
        "    fitter.load('models_HVT/best-checkpoint.bin')\n",
        "    \n",
        "    predictions = fitter.predict(test_loader)\n",
        "\n",
        "    wandb.finish()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset bloodmnist Task multi-class n_channels 3 n_classes 8\n",
            "Using downloaded and verified file: /root/.medmnist/bloodmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/bloodmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/bloodmnist.npz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X train (11959, 32, 32, 3) | Y train (11959, 1)\n",
            "X val (1712, 32, 32, 3) | Y val (1712, 1)\n",
            "X test (3421, 32, 32, 3) | Y test (3421, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:35b4pxva) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 2178... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4e336f086a64111bc09c6d9081fbc2f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "</div><div class=\"wandb-col\">\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">rich-water-53</strong>: <a href=\"https://wandb.ai/ual/hvit_benchmark_bloodmnist/runs/35b4pxva\" target=\"_blank\">https://wandb.ai/ual/hvit_benchmark_bloodmnist/runs/35b4pxva</a><br/>\n",
              "Find logs at: <code>./wandb/run-20211114_230853-35b4pxva/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Successfully finished last run (ID:35b4pxva). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/ual/hvit_benchmark_bloodmnist/runs/2nruavc0\" target=\"_blank\">summer-cherry-54</a></strong> to <a href=\"https://wandb.ai/ual/hvit_benchmark_bloodmnist\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HVT(\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
            "  )\n",
            "  (pos_drop): Dropout(p=0.2, inplace=False)\n",
            "  (blocks): ModuleList(\n",
            "    (0): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (downsample): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (1): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (2): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (3): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (4): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (5): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (6): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (7): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (8): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (9): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (10): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (11): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "  (head): Linear(in_features=384, out_features=8, bias=True)\n",
            ")\n",
            "number of params: 21594632\n",
            "Fitter prepared. Device is cuda\n",
            "\n",
            "2021-11-14 23:10:15\n",
            "                         EPOCH 1/100 - LR: 0.0005\n",
            "[TRAIN] 8.00s - train loss: 2.06045\n",
            "[VALIDATION] 0.36s - val. loss: 2.03988 - accuracy_score 0.18224 \n",
            "[RESULT] 8.37s - train loss: 2.06045 - val loss: 2.03988 - f1_score: 0.038537549407114624 - accuracy_score: 0.1822429906542056\n",
            "Validation metric improved from -inf to 0.038537549407114624\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:10:23\n",
            "                         EPOCH 2/100 - LR: 0.0005\n",
            "[TRAIN] 8.01s - train loss: 2.03571\n",
            "[VALIDATION] 0.36s - val. loss: 2.02672 - accuracy_score 0.18224 \n",
            "[RESULT] 8.38s - train loss: 2.03571 - val loss: 2.02672 - f1_score: 0.038537549407114624 - accuracy_score: 0.1822429906542056\n",
            "\n",
            "2021-11-14 23:10:32\n",
            "                         EPOCH 3/100 - LR: 0.0005\n",
            "[TRAIN] 7.71s - train loss: 2.02382\n",
            "[VALIDATION] 0.37s - val. loss: 2.16607 - accuracy_score 0.18224 \n",
            "[RESULT] 8.08s - train loss: 2.02382 - val loss: 2.16607 - f1_score: 0.038537549407114624 - accuracy_score: 0.1822429906542056\n",
            "\n",
            "2021-11-14 23:10:40\n",
            "                         EPOCH 4/100 - LR: 0.0005\n",
            "[TRAIN] 7.58s - train loss: 2.01568\n",
            "[VALIDATION] 0.39s - val. loss: 2.04045 - accuracy_score 0.13727 \n",
            "[RESULT] 7.98s - train loss: 2.01568 - val loss: 2.04045 - f1_score: 0.03017462763225475 - accuracy_score: 0.13726635514018692\n",
            "\n",
            "2021-11-14 23:10:48\n",
            "                         EPOCH 5/100 - LR: 0.0005\n",
            "[TRAIN] 7.87s - train loss: 1.92577\n",
            "[VALIDATION] 0.37s - val. loss: 1.71130 - accuracy_score 0.39603 \n",
            "[RESULT] 8.25s - train loss: 1.92577 - val loss: 1.71130 - f1_score: 0.22779791141457914 - accuracy_score: 0.39602803738317754\n",
            "Validation metric improved from 0.038537549407114624 to 0.22779791141457914\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:10:57\n",
            "                         EPOCH 6/100 - LR: 0.0005\n",
            "[TRAIN] 8.27s - train loss: 1.58065\n",
            "[VALIDATION] 0.38s - val. loss: 1.57191 - accuracy_score 0.43516 \n",
            "[RESULT] 8.66s - train loss: 1.58065 - val loss: 1.57191 - f1_score: 0.32563773856027545 - accuracy_score: 0.4351635514018692\n",
            "Validation metric improved from 0.22779791141457914 to 0.32563773856027545\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:11:06\n",
            "                         EPOCH 7/100 - LR: 0.0005\n",
            "[TRAIN] 7.89s - train loss: 1.52557\n",
            "[VALIDATION] 0.36s - val. loss: 1.53382 - accuracy_score 0.46028 \n",
            "[RESULT] 8.26s - train loss: 1.52557 - val loss: 1.53382 - f1_score: 0.32159347331679444 - accuracy_score: 0.4602803738317757\n",
            "\n",
            "2021-11-14 23:11:14\n",
            "                         EPOCH 8/100 - LR: 0.0005\n",
            "[TRAIN] 7.61s - train loss: 1.47837\n",
            "[VALIDATION] 0.38s - val. loss: 1.46401 - accuracy_score 0.50117 \n",
            "[RESULT] 7.99s - train loss: 1.47837 - val loss: 1.46401 - f1_score: 0.3549817886153689 - accuracy_score: 0.5011682242990654\n",
            "Validation metric improved from 0.32563773856027545 to 0.3549817886153689\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:11:22\n",
            "                         EPOCH 9/100 - LR: 0.0005\n",
            "[TRAIN] 8.12s - train loss: 1.46043\n",
            "[VALIDATION] 0.39s - val. loss: 1.42458 - accuracy_score 0.53096 \n",
            "[RESULT] 8.52s - train loss: 1.46043 - val loss: 1.42458 - f1_score: 0.4200998130897685 - accuracy_score: 0.5309579439252337\n",
            "Validation metric improved from 0.3549817886153689 to 0.4200998130897685\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:11:31\n",
            "                         EPOCH 10/100 - LR: 0.0005\n",
            "[TRAIN] 8.08s - train loss: 1.41262\n",
            "[VALIDATION] 0.37s - val. loss: 1.33877 - accuracy_score 0.56542 \n",
            "[RESULT] 8.46s - train loss: 1.41262 - val loss: 1.33877 - f1_score: 0.4717147591439616 - accuracy_score: 0.5654205607476636\n",
            "Validation metric improved from 0.4200998130897685 to 0.4717147591439616\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:11:40\n",
            "                         EPOCH 11/100 - LR: 0.0005\n",
            "[TRAIN] 7.71s - train loss: 1.33999\n",
            "[VALIDATION] 0.37s - val. loss: 1.24774 - accuracy_score 0.60923 \n",
            "[RESULT] 8.09s - train loss: 1.33999 - val loss: 1.24774 - f1_score: 0.49958976577243913 - accuracy_score: 0.6092289719626168\n",
            "Validation metric improved from 0.4717147591439616 to 0.49958976577243913\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:11:49\n",
            "                         EPOCH 12/100 - LR: 0.0005\n",
            "[TRAIN] 7.94s - train loss: 1.26185\n",
            "[VALIDATION] 0.37s - val. loss: 1.25494 - accuracy_score 0.59171 \n",
            "[RESULT] 8.31s - train loss: 1.26185 - val loss: 1.25494 - f1_score: 0.5128590475964 - accuracy_score: 0.5917056074766355\n",
            "Validation metric improved from 0.49958976577243913 to 0.5128590475964\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:11:58\n",
            "                         EPOCH 13/100 - LR: 0.0005\n",
            "[TRAIN] 7.92s - train loss: 1.20993\n",
            "[VALIDATION] 0.40s - val. loss: 1.15619 - accuracy_score 0.64136 \n",
            "[RESULT] 8.32s - train loss: 1.20993 - val loss: 1.15619 - f1_score: 0.6023261955162194 - accuracy_score: 0.6413551401869159\n",
            "Validation metric improved from 0.5128590475964 to 0.6023261955162194\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:12:07\n",
            "                         EPOCH 14/100 - LR: 0.0005\n",
            "[TRAIN] 7.65s - train loss: 1.19626\n",
            "[VALIDATION] 0.38s - val. loss: 1.14589 - accuracy_score 0.66939 \n",
            "[RESULT] 8.04s - train loss: 1.19626 - val loss: 1.14589 - f1_score: 0.6120777713931911 - accuracy_score: 0.669392523364486\n",
            "Validation metric improved from 0.6023261955162194 to 0.6120777713931911\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:12:15\n",
            "                         EPOCH 15/100 - LR: 0.0005\n",
            "[TRAIN] 7.43s - train loss: 1.15001\n",
            "[VALIDATION] 0.39s - val. loss: 1.10469 - accuracy_score 0.67815 \n",
            "[RESULT] 7.82s - train loss: 1.15001 - val loss: 1.10469 - f1_score: 0.6304299894803481 - accuracy_score: 0.6781542056074766\n",
            "Validation metric improved from 0.6120777713931911 to 0.6304299894803481\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:12:23\n",
            "                         EPOCH 16/100 - LR: 0.0005\n",
            "[TRAIN] 7.90s - train loss: 1.14006\n",
            "[VALIDATION] 0.39s - val. loss: 1.07430 - accuracy_score 0.70386 \n",
            "[RESULT] 8.29s - train loss: 1.14006 - val loss: 1.07430 - f1_score: 0.6747744065301602 - accuracy_score: 0.7038551401869159\n",
            "Validation metric improved from 0.6304299894803481 to 0.6747744065301602\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:12:32\n",
            "                         EPOCH 17/100 - LR: 0.0005\n",
            "[TRAIN] 7.77s - train loss: 1.12249\n",
            "[VALIDATION] 0.43s - val. loss: 1.08311 - accuracy_score 0.69334 \n",
            "[RESULT] 8.20s - train loss: 1.12249 - val loss: 1.08311 - f1_score: 0.6360162260299967 - accuracy_score: 0.6933411214953271\n",
            "\n",
            "2021-11-14 23:12:40\n",
            "                         EPOCH 18/100 - LR: 0.0005\n",
            "[TRAIN] 7.85s - train loss: 1.09903\n",
            "[VALIDATION] 0.40s - val. loss: 1.07381 - accuracy_score 0.70619 \n",
            "[RESULT] 8.25s - train loss: 1.09903 - val loss: 1.07381 - f1_score: 0.6642250944172639 - accuracy_score: 0.7061915887850467\n",
            "\n",
            "2021-11-14 23:12:49\n",
            "                         EPOCH 19/100 - LR: 0.0005\n",
            "[TRAIN] 7.79s - train loss: 1.08978\n",
            "[VALIDATION] 0.38s - val. loss: 1.04211 - accuracy_score 0.72371 \n",
            "[RESULT] 8.17s - train loss: 1.08978 - val loss: 1.04211 - f1_score: 0.6908899816754347 - accuracy_score: 0.7237149532710281\n",
            "Validation metric improved from 0.6747744065301602 to 0.6908899816754347\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:12:57\n",
            "                         EPOCH 20/100 - LR: 0.0005\n",
            "[TRAIN] 7.75s - train loss: 1.07342\n",
            "[VALIDATION] 0.38s - val. loss: 1.00685 - accuracy_score 0.73423 \n",
            "[RESULT] 8.14s - train loss: 1.07342 - val loss: 1.00685 - f1_score: 0.6973933300796741 - accuracy_score: 0.7342289719626168\n",
            "Validation metric improved from 0.6908899816754347 to 0.6973933300796741\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:13:06\n",
            "                         EPOCH 21/100 - LR: 0.0005\n",
            "[TRAIN] 7.91s - train loss: 1.05794\n",
            "[VALIDATION] 0.38s - val. loss: 1.02802 - accuracy_score 0.72897 \n",
            "[RESULT] 8.29s - train loss: 1.05794 - val loss: 1.02802 - f1_score: 0.699613946619622 - accuracy_score: 0.7289719626168224\n",
            "Validation metric improved from 0.6973933300796741 to 0.699613946619622\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:13:15\n",
            "                         EPOCH 22/100 - LR: 0.0005\n",
            "[TRAIN] 7.74s - train loss: 1.05021\n",
            "[VALIDATION] 0.36s - val. loss: 1.06742 - accuracy_score 0.71612 \n",
            "[RESULT] 8.10s - train loss: 1.05021 - val loss: 1.06742 - f1_score: 0.6774749863069721 - accuracy_score: 0.7161214953271028\n",
            "\n",
            "2021-11-14 23:13:23\n",
            "                         EPOCH 23/100 - LR: 0.0005\n",
            "[TRAIN] 7.92s - train loss: 1.04727\n",
            "[VALIDATION] 0.38s - val. loss: 1.05298 - accuracy_score 0.71203 \n",
            "[RESULT] 8.31s - train loss: 1.04727 - val loss: 1.05298 - f1_score: 0.659269200812417 - accuracy_score: 0.7120327102803738\n",
            "\n",
            "2021-11-14 23:13:31\n",
            "                         EPOCH 24/100 - LR: 0.0005\n",
            "[TRAIN] 7.58s - train loss: 1.03658\n",
            "[VALIDATION] 0.37s - val. loss: 1.01302 - accuracy_score 0.71028 \n",
            "[RESULT] 7.96s - train loss: 1.03658 - val loss: 1.01302 - f1_score: 0.6320776939475208 - accuracy_score: 0.7102803738317757\n",
            "\n",
            "2021-11-14 23:13:39\n",
            "                         EPOCH 25/100 - LR: 0.0005\n",
            "[TRAIN] 7.66s - train loss: 1.02014\n",
            "[VALIDATION] 0.38s - val. loss: 0.97254 - accuracy_score 0.75000 \n",
            "[RESULT] 8.05s - train loss: 1.02014 - val loss: 0.97254 - f1_score: 0.723026963803541 - accuracy_score: 0.75\n",
            "Validation metric improved from 0.699613946619622 to 0.723026963803541\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:13:48\n",
            "                         EPOCH 26/100 - LR: 0.0005\n",
            "[TRAIN] 7.88s - train loss: 1.00111\n",
            "[VALIDATION] 0.37s - val. loss: 0.96330 - accuracy_score 0.76402 \n",
            "[RESULT] 8.25s - train loss: 1.00111 - val loss: 0.96330 - f1_score: 0.7365035064075355 - accuracy_score: 0.764018691588785\n",
            "Validation metric improved from 0.723026963803541 to 0.7365035064075355\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:13:56\n",
            "                         EPOCH 27/100 - LR: 0.0005\n",
            "[TRAIN] 7.94s - train loss: 0.99878\n",
            "[VALIDATION] 0.40s - val. loss: 0.96465 - accuracy_score 0.75350 \n",
            "[RESULT] 8.34s - train loss: 0.99878 - val loss: 0.96465 - f1_score: 0.7254168788126769 - accuracy_score: 0.7535046728971962\n",
            "\n",
            "2021-11-14 23:14:05\n",
            "                         EPOCH 28/100 - LR: 0.0005\n",
            "[TRAIN] 7.38s - train loss: 0.99493\n",
            "[VALIDATION] 0.35s - val. loss: 0.97848 - accuracy_score 0.74708 \n",
            "[RESULT] 7.74s - train loss: 0.99493 - val loss: 0.97848 - f1_score: 0.7104895170302403 - accuracy_score: 0.7470794392523364\n",
            "\n",
            "2021-11-14 23:14:13\n",
            "                         EPOCH 29/100 - LR: 0.0005\n",
            "[TRAIN] 7.73s - train loss: 1.01805\n",
            "[VALIDATION] 0.37s - val. loss: 0.95192 - accuracy_score 0.76402 \n",
            "[RESULT] 8.11s - train loss: 1.01805 - val loss: 0.95192 - f1_score: 0.7195739796479524 - accuracy_score: 0.764018691588785\n",
            "\n",
            "2021-11-14 23:14:21\n",
            "                         EPOCH 30/100 - LR: 0.0005\n",
            "[TRAIN] 7.86s - train loss: 0.97756\n",
            "[VALIDATION] 0.37s - val. loss: 0.97973 - accuracy_score 0.74591 \n",
            "[RESULT] 8.23s - train loss: 0.97756 - val loss: 0.97973 - f1_score: 0.7121981812155957 - accuracy_score: 0.7459112149532711\n",
            "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
            "\n",
            "2021-11-14 23:14:29\n",
            "                         EPOCH 31/100 - LR: 0.0001\n",
            "[TRAIN] 7.64s - train loss: 0.92474\n",
            "[VALIDATION] 0.39s - val. loss: 0.89847 - accuracy_score 0.78855 \n",
            "[RESULT] 8.04s - train loss: 0.92474 - val loss: 0.89847 - f1_score: 0.7557785029240185 - accuracy_score: 0.7885514018691588\n",
            "Validation metric improved from 0.7365035064075355 to 0.7557785029240185\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:14:37\n",
            "                         EPOCH 32/100 - LR: 0.0001\n",
            "[TRAIN] 7.98s - train loss: 0.90499\n",
            "[VALIDATION] 0.41s - val. loss: 0.89712 - accuracy_score 0.78563 \n",
            "[RESULT] 8.40s - train loss: 0.90499 - val loss: 0.89712 - f1_score: 0.7578571406982737 - accuracy_score: 0.7856308411214953\n",
            "Validation metric improved from 0.7557785029240185 to 0.7578571406982737\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:14:46\n",
            "                         EPOCH 33/100 - LR: 0.0001\n",
            "[TRAIN] 7.73s - train loss: 0.89910\n",
            "[VALIDATION] 0.38s - val. loss: 0.88763 - accuracy_score 0.79439 \n",
            "[RESULT] 8.11s - train loss: 0.89910 - val loss: 0.88763 - f1_score: 0.765394148657497 - accuracy_score: 0.794392523364486\n",
            "Validation metric improved from 0.7578571406982737 to 0.765394148657497\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:14:55\n",
            "                         EPOCH 34/100 - LR: 0.0001\n",
            "[TRAIN] 7.72s - train loss: 0.89423\n",
            "[VALIDATION] 0.38s - val. loss: 0.88416 - accuracy_score 0.79498 \n",
            "[RESULT] 8.10s - train loss: 0.89423 - val loss: 0.88416 - f1_score: 0.77076724939936 - accuracy_score: 0.7949766355140186\n",
            "Validation metric improved from 0.765394148657497 to 0.77076724939936\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:15:04\n",
            "                         EPOCH 35/100 - LR: 0.0001\n",
            "[TRAIN] 8.06s - train loss: 0.88924\n",
            "[VALIDATION] 0.39s - val. loss: 0.87803 - accuracy_score 0.80491 \n",
            "[RESULT] 8.46s - train loss: 0.88924 - val loss: 0.87803 - f1_score: 0.7799662012938438 - accuracy_score: 0.8049065420560748\n",
            "Validation metric improved from 0.77076724939936 to 0.7799662012938438\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:15:12\n",
            "                         EPOCH 36/100 - LR: 0.0001\n",
            "[TRAIN] 7.83s - train loss: 0.88325\n",
            "[VALIDATION] 0.35s - val. loss: 0.87363 - accuracy_score 0.80140 \n",
            "[RESULT] 8.18s - train loss: 0.88325 - val loss: 0.87363 - f1_score: 0.7759457811014518 - accuracy_score: 0.8014018691588785\n",
            "\n",
            "2021-11-14 23:15:21\n",
            "                         EPOCH 37/100 - LR: 0.0001\n",
            "[TRAIN] 7.75s - train loss: 0.88642\n",
            "[VALIDATION] 0.40s - val. loss: 0.86886 - accuracy_score 0.79790 \n",
            "[RESULT] 8.15s - train loss: 0.88642 - val loss: 0.86886 - f1_score: 0.7722193234874164 - accuracy_score: 0.7978971962616822\n",
            "\n",
            "2021-11-14 23:15:29\n",
            "                         EPOCH 38/100 - LR: 0.0001\n",
            "[TRAIN] 7.70s - train loss: 0.87964\n",
            "[VALIDATION] 0.38s - val. loss: 0.88694 - accuracy_score 0.79206 \n",
            "[RESULT] 8.09s - train loss: 0.87964 - val loss: 0.88694 - f1_score: 0.7686237876965989 - accuracy_score: 0.7920560747663551\n",
            "\n",
            "2021-11-14 23:15:37\n",
            "                         EPOCH 39/100 - LR: 0.0001\n",
            "[TRAIN] 8.02s - train loss: 0.86897\n",
            "[VALIDATION] 0.36s - val. loss: 0.87134 - accuracy_score 0.80023 \n",
            "[RESULT] 8.39s - train loss: 0.86897 - val loss: 0.87134 - f1_score: 0.7704799055401431 - accuracy_score: 0.8002336448598131\n",
            "Epoch    39: reducing learning rate of group 0 to 2.0000e-05.\n",
            "\n",
            "2021-11-14 23:15:45\n",
            "                         EPOCH 40/100 - LR: 2e-05\n",
            "[TRAIN] 7.60s - train loss: 0.85732\n",
            "[VALIDATION] 0.37s - val. loss: 0.85203 - accuracy_score 0.81075 \n",
            "[RESULT] 7.97s - train loss: 0.85732 - val loss: 0.85203 - f1_score: 0.7889407346911886 - accuracy_score: 0.8107476635514018\n",
            "Validation metric improved from 0.7799662012938438 to 0.7889407346911886\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:15:54\n",
            "                         EPOCH 41/100 - LR: 2e-05\n",
            "[TRAIN] 7.95s - train loss: 0.85507\n",
            "[VALIDATION] 0.39s - val. loss: 0.85106 - accuracy_score 0.81250 \n",
            "[RESULT] 8.34s - train loss: 0.85507 - val loss: 0.85106 - f1_score: 0.7884532435085219 - accuracy_score: 0.8125\n",
            "\n",
            "2021-11-14 23:16:02\n",
            "                         EPOCH 42/100 - LR: 2e-05\n",
            "[TRAIN] 7.72s - train loss: 0.85151\n",
            "[VALIDATION] 0.42s - val. loss: 0.85209 - accuracy_score 0.81308 \n",
            "[RESULT] 8.15s - train loss: 0.85151 - val loss: 0.85209 - f1_score: 0.7896895179964609 - accuracy_score: 0.8130841121495327\n",
            "Validation metric improved from 0.7889407346911886 to 0.7896895179964609\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:16:11\n",
            "                         EPOCH 43/100 - LR: 2e-05\n",
            "[TRAIN] 7.91s - train loss: 0.84872\n",
            "[VALIDATION] 0.37s - val. loss: 0.84699 - accuracy_score 0.81367 \n",
            "[RESULT] 8.29s - train loss: 0.84872 - val loss: 0.84699 - f1_score: 0.7908406003695301 - accuracy_score: 0.8136682242990654\n",
            "Validation metric improved from 0.7896895179964609 to 0.7908406003695301\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:16:20\n",
            "                         EPOCH 44/100 - LR: 2e-05\n",
            "[TRAIN] 7.78s - train loss: 0.84844\n",
            "[VALIDATION] 0.35s - val. loss: 0.84648 - accuracy_score 0.81425 \n",
            "[RESULT] 8.14s - train loss: 0.84844 - val loss: 0.84648 - f1_score: 0.7883837063893282 - accuracy_score: 0.8142523364485982\n",
            "\n",
            "2021-11-14 23:16:28\n",
            "                         EPOCH 45/100 - LR: 2e-05\n",
            "[TRAIN] 7.94s - train loss: 0.85109\n",
            "[VALIDATION] 0.38s - val. loss: 0.84559 - accuracy_score 0.81542 \n",
            "[RESULT] 8.32s - train loss: 0.85109 - val loss: 0.84559 - f1_score: 0.7898272688122854 - accuracy_score: 0.8154205607476636\n",
            "\n",
            "2021-11-14 23:16:36\n",
            "                         EPOCH 46/100 - LR: 2e-05\n",
            "[TRAIN] 7.49s - train loss: 0.84547\n",
            "[VALIDATION] 0.38s - val. loss: 0.84448 - accuracy_score 0.81367 \n",
            "[RESULT] 7.88s - train loss: 0.84547 - val loss: 0.84448 - f1_score: 0.7898104485135524 - accuracy_score: 0.8136682242990654\n",
            "\n",
            "2021-11-14 23:16:44\n",
            "                         EPOCH 47/100 - LR: 2e-05\n",
            "[TRAIN] 7.69s - train loss: 0.84676\n",
            "[VALIDATION] 0.38s - val. loss: 0.84752 - accuracy_score 0.81659 \n",
            "[RESULT] 8.07s - train loss: 0.84676 - val loss: 0.84752 - f1_score: 0.7914200633730029 - accuracy_score: 0.8165887850467289\n",
            "Validation metric improved from 0.7908406003695301 to 0.7914200633730029\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:16:52\n",
            "                         EPOCH 48/100 - LR: 2e-05\n",
            "[TRAIN] 7.57s - train loss: 0.84458\n",
            "[VALIDATION] 0.39s - val. loss: 0.84129 - accuracy_score 0.82009 \n",
            "[RESULT] 7.96s - train loss: 0.84458 - val loss: 0.84129 - f1_score: 0.7982037716892617 - accuracy_score: 0.8200934579439252\n",
            "Validation metric improved from 0.7914200633730029 to 0.7982037716892617\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:17:01\n",
            "                         EPOCH 49/100 - LR: 2e-05\n",
            "[TRAIN] 8.04s - train loss: 0.84000\n",
            "[VALIDATION] 0.39s - val. loss: 0.84262 - accuracy_score 0.81776 \n",
            "[RESULT] 8.43s - train loss: 0.84000 - val loss: 0.84262 - f1_score: 0.793809211181262 - accuracy_score: 0.8177570093457944\n",
            "\n",
            "2021-11-14 23:17:09\n",
            "                         EPOCH 50/100 - LR: 2e-05\n",
            "[TRAIN] 7.76s - train loss: 0.84215\n",
            "[VALIDATION] 0.39s - val. loss: 0.83961 - accuracy_score 0.82009 \n",
            "[RESULT] 8.15s - train loss: 0.84215 - val loss: 0.83961 - f1_score: 0.7985239319229243 - accuracy_score: 0.8200934579439252\n",
            "Validation metric improved from 0.7982037716892617 to 0.7985239319229243\n",
            "Checkpoint is saved to models_HVT/best-checkpoint.bin\n",
            "\n",
            "2021-11-14 23:17:18\n",
            "                         EPOCH 51/100 - LR: 2e-05\n",
            "[TRAIN] 7.50s - train loss: 0.83973\n",
            "[VALIDATION] 0.38s - val. loss: 0.84219 - accuracy_score 0.81893 \n",
            "[RESULT] 7.89s - train loss: 0.83973 - val loss: 0.84219 - f1_score: 0.792472790982876 - accuracy_score: 0.8189252336448598\n",
            "\n",
            "2021-11-14 23:17:26\n",
            "                         EPOCH 52/100 - LR: 2e-05\n",
            "[TRAIN] 7.57s - train loss: 0.83649\n",
            "[VALIDATION] 0.38s - val. loss: 0.83976 - accuracy_score 0.81484 \n",
            "[RESULT] 7.95s - train loss: 0.83649 - val loss: 0.83976 - f1_score: 0.7915292678163457 - accuracy_score: 0.8148364485981309\n",
            "\n",
            "2021-11-14 23:17:34\n",
            "                         EPOCH 53/100 - LR: 2e-05\n",
            "[TRAIN] 7.81s - train loss: 0.83996\n",
            "[VALIDATION] 0.38s - val. loss: 0.83892 - accuracy_score 0.81834 \n",
            "[RESULT] 8.19s - train loss: 0.83996 - val loss: 0.83892 - f1_score: 0.79347467157775 - accuracy_score: 0.8183411214953271\n",
            "\n",
            "2021-11-14 23:17:42\n",
            "                         EPOCH 54/100 - LR: 2e-05\n",
            "[TRAIN] 7.72s - train loss: 0.83621\n",
            "[VALIDATION] 0.36s - val. loss: 0.84467 - accuracy_score 0.82185 \n",
            "[RESULT] 8.09s - train loss: 0.83621 - val loss: 0.84467 - f1_score: 0.7964602552096156 - accuracy_score: 0.8218457943925234\n",
            "Epoch    54: reducing learning rate of group 0 to 4.0000e-06.\n",
            "\n",
            "2021-11-14 23:17:50\n",
            "                         EPOCH 55/100 - LR: 4.000000000000001e-06\n",
            "[TRAIN] 7.80s - train loss: 0.83394\n",
            "[VALIDATION] 0.37s - val. loss: 0.83589 - accuracy_score 0.82009 \n",
            "[RESULT] 8.18s - train loss: 0.83394 - val loss: 0.83589 - f1_score: 0.7977724476669853 - accuracy_score: 0.8200934579439252\n",
            "\n",
            "2021-11-14 23:17:58\n",
            "                         EPOCH 56/100 - LR: 4.000000000000001e-06\n",
            "[TRAIN] 7.75s - train loss: 0.82642\n",
            "[VALIDATION] 0.41s - val. loss: 0.83608 - accuracy_score 0.81600 \n",
            "[RESULT] 8.16s - train loss: 0.82642 - val loss: 0.83608 - f1_score: 0.793876486185989 - accuracy_score: 0.8160046728971962\n",
            "\n",
            "2021-11-14 23:18:06\n",
            "                         EPOCH 57/100 - LR: 4.000000000000001e-06\n",
            "[TRAIN] 7.54s - train loss: 0.83241\n",
            "[VALIDATION] 0.39s - val. loss: 0.83688 - accuracy_score 0.82009 \n",
            "[RESULT] 7.93s - train loss: 0.83241 - val loss: 0.83688 - f1_score: 0.7973553923132386 - accuracy_score: 0.8200934579439252\n",
            "Early Stopping: 7 epochs with no improvement\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 2259... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de5fb7a391784b75ad1affbbea99a8ab",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "</div><div class=\"wandb-col\">\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">summer-cherry-54</strong>: <a href=\"https://wandb.ai/ual/hvit_benchmark_bloodmnist/runs/2nruavc0\" target=\"_blank\">https://wandb.ai/ual/hvit_benchmark_bloodmnist/runs/2nruavc0</a><br/>\n",
              "Find logs at: <code>./wandb/run-20211114_231008-2nruavc0/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}